{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3631159",
   "metadata": {},
   "source": [
    "Obrazy z życia wzięte (zbiory danych) nie wyglądają jak FASHION MNIST - w prawdziwym życiu mielibyśmy obrazy o różnych wysokościach i szerokościach, w znacznie większej rozdzielczości. Spójrz na zdjęcia z poniższych katalogów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"data/alien-vs-predator/train/\"\n",
    "validation_dir = \"data/alien-vs-predator/validation/\"\n",
    "test_dir = \"data/alien-vs-predator/test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5ce38",
   "metadata": {},
   "source": [
    "W przypadku zbioru danych \"alien vs predator\" musielibyśmy zaimportować wszystkie obrazy do `Python` i przekształcić je w odpowiednie tensory, aby zbudować model w `keras`. Teraz prawdopodobnie widzisz wąskie gardło - `Python` jest wolny, więc gdybyśmy mieli na przykład miliony obrazów, zajęłoby to całe wieki i prawdopodobnie zabrakłoby nam pamięci. Na szczęście w `keras` istnieje sposób na uniknięcie wczytywania całych danych do `Python` - możemy użyć **generatorów danych** i **przepływów**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae03944",
   "metadata": {},
   "source": [
    "Zaczniemy od stworzenia prostego generatora danych dla zbioru treningowego i walidacyjnego, który powie `keras` jak przekształcać obrazy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5038269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78856f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f1f27",
   "metadata": {},
   "source": [
    "W kolejnym kroku musimy stworzyć flow obrazu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow = train_datagen.flow_from_directory(\n",
    "  directory = train_dir, # Path for train images folder\n",
    "  color_mode = \"rgb\", # Images are in color\n",
    "  target_size = (150, 150), # Scale all images to 150x150\n",
    "  batch_size = 32, # Batch size\n",
    "  class_mode = \"categorical\" # Classification task\n",
    ")\n",
    "\n",
    "validation_flow = validation_datagen.flow_from_directory(\n",
    "  directory = validation_dir,\n",
    "  color_mode = \"rgb\",\n",
    "  target_size = (150, 150),\n",
    "  batch_size = 32,\n",
    "  class_mode = \"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243132dc",
   "metadata": {},
   "source": [
    "Jeśli chcemy, możemy sprawdzić przykładowe obrazy z naszego flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = train_flow.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 32):\n",
    "    img = sample_batch[0][i, :, : , :]\n",
    "    plt.matshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc74df9",
   "metadata": {},
   "source": [
    "Teraz czas na zbudowanie naszego pierwszego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zbuduj model\n",
    "\n",
    "alien_predator_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79650599",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_predator_model_1.compile(\n",
    "    optimizer = tf.keras.optimizers.RMSprop(),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = (\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = alien_predator_model_1.fit(\n",
    "        train_flow,\n",
    "        steps_per_epoch=22, # ceiling(694 / 32)\n",
    "        epochs=15,\n",
    "        validation_data=validation_flow,\n",
    "        validation_steps=6) # ceiling(184 / 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a47b0",
   "metadata": {},
   "source": [
    "W podobny sposób możemy ocenić model na zbiorze testowym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "\n",
    "test_flow = train_datagen.flow_from_directory(\n",
    "  directory = test_dir, # Path for train images folder\n",
    "  color_mode = \"rgb\", # Images are in color\n",
    "  target_size = (150, 150), # Scale all images to 150x150\n",
    "  batch_size = 1, # Batch size\n",
    "  class_mode = \"categorical\" # Classification task\n",
    ")\n",
    "\n",
    "alien_predator_model_1.evaluate(test_flow, steps = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5065f282",
   "metadata": {},
   "source": [
    "Jak widać, nasz model daleki jest od doskonałości. Nauczmy się nowych sztuczek. Jak zapewne wiesz, jeśli wielkość twojej próbki jest mała, najlepszą rzeczą, jaką możesz zrobić, jest jej zwiększenie. W naszym przypadku moglibyśmy zebrać więcej zdjęć, ale co zrobić, jeśli to niemożliwe? Możemy generować nowe próbki w procesie **augmentacji danych**. Na szczęście dla nas jest to bardzo proste, musimy tylko dodać kilka dodatkowych argumentów do generatora danych treningowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d023fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1 / 255,\n",
    "    rotation_range = 35,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "\n",
    "train_flow = train_datagen.flow_from_directory(\n",
    "  directory = train_dir, # Path for train images folder\n",
    "  color_mode = \"rgb\", # Images are in color\n",
    "  target_size = (150, 150), # Scale all images to 150x150\n",
    "  batch_size = 32, # Batch size\n",
    "  class_mode = \"categorical\" # Classification task\n",
    ")\n",
    "\n",
    "validation_flow = validation_datagen.flow_from_directory(\n",
    "  directory = validation_dir,\n",
    "  color_mode = \"rgb\",\n",
    "  target_size = (150, 150),\n",
    "  batch_size = 32,\n",
    "  class_mode = \"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18108478",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = train_flow.next()\n",
    "\n",
    "for i in range(0, 32):\n",
    "    img = sample_batch[0][i, :, : , :]\n",
    "    plt.matshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6222da9",
   "metadata": {},
   "source": [
    "Od tego momentu budowanie architektury CNN i dopasowywanie modelu wyglądałoby dokładnie tak samo, ale jest jeszcze jedna bardzo potężna metoda, której możemy użyć do stworzenia lepszego modelu. Użyjemy **fine-tuningu**, będącego jedną z wielu metod z pola **transfer learning**. W krótkim podsumowaniu, jeśli masz wstępnie wytrenowany model dopasowany do dużego zbioru danych, który jest w pewien sposób podobny do innych danych, możesz dostroić ten model do pracy na innych danych.\n",
    "\n",
    "Aby dokonać fine-tuningu w `keras`, musimy zacząć od wcześniej wytrenowanego modelu. W `keras` mamy dostęp do kilku różnych architektur wstępnie przeszkolonych w zbiorze danych **ImageNet** zawierającym miliony obrazów z ponad 1000 klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = tf.keras.applications.vgg16.VGG16(\n",
    "  weights = \"imagenet\", # Weights trained on 'imagenet'\n",
    "  include_top = False, # Without dense layers on top - we will add them later\n",
    "  input_shape = (150, 150, 3) # Same shape as in our generators\n",
    ")\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef6157",
   "metadata": {},
   "source": [
    "Jak pamiętacie, filtry CNN w pierwszych warstwach przedstawiają podstawowe zmienne, takie jak linie, krzywe itp. Te zmienne będą przydatne w naszym tuningowanym modelu, więc CNN nie musi uczyć się tego od nowa. Będziemy interesować się tylko zmiennymi w kilku ostatnich warstwach, które reprezentują specyficzne cechy dla naszego zadania. Na początku musimy zamrozić wagi CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161a5f0",
   "metadata": {},
   "source": [
    "W następnym kroku musimy dodać warstwę wyjściową (i dodatkowe warstwy, jeśli chcemy) na wierzchu bazy konwolucyjnej i skompilować cały model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8325a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(150, 150, 3))\n",
    "outputs = conv_base(inputs, training=False)\n",
    "outputs = tf.keras.layers.Flatten()(outputs)\n",
    "outputs = tf.keras.layers.Dense(units = 256, activation = \"relu\")(outputs)\n",
    "outputs = tf.keras.layers.Dense(units = 2, activation = \"softmax\")(outputs)\n",
    "alien_predator_model_2 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "alien_predator_model_2.compile(\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr = 1e-5),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = (\"accuracy\"))\n",
    "\n",
    "alien_predator_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = alien_predator_model_2.fit(\n",
    "        train_flow,\n",
    "        steps_per_epoch=22, # ceiling(694 / 32)\n",
    "        epochs=15,\n",
    "        validation_data=validation_flow,\n",
    "        validation_steps=6) # ceiling(184 / 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7da777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "\n",
    "test_flow = train_datagen.flow_from_directory(\n",
    "  directory = test_dir, # Path for train images folder\n",
    "  color_mode = \"rgb\", # Images are in color\n",
    "  target_size = (150, 150), # Scale all images to 150x150\n",
    "  batch_size = 1, # Batch size\n",
    "  class_mode = \"categorical\" # Classification task\n",
    ")\n",
    "\n",
    "alien_predator_model_2.evaluate(test_flow, steps = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8a5b1",
   "metadata": {},
   "source": [
    "# Praca domowa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ac087",
   "metadata": {},
   "source": [
    "Teraz twoja kolej! Utwórz CNN za pomocą generatora danych i przepływów, aby sklasyfikować obrazy gestów języka migowego. Tym razem nie stosuj fine-tuningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a479065",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/sign-language-mnist/train/\"\n",
    "test_path = \"data/sign-language-mnist/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6150bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
