{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0233db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95cac5",
   "metadata": {},
   "source": [
    "Zacznijmy od prostej funkcji `f (x) = x ^ 2 + 1` i jej pochodnej `f'(x) = 2x`. Znalezienie minimum `f(x)` jest proste w tej sytuacji: minimalna wartość jest równa `1`, gdy `x == 0`. Zauważ, że nasza pochodna `f'(x)` równa się `0` dokładnie w tym punkcie (jeśli wartość pochodnej `f'(x) `jest równa `0` w punkcie `x == x0`, funkcja `f(x)` ma lokalne / globalne minimum / maksimum / punkt zwrotny w `x0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ceb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2 + 1\n",
    "grad_f = lambda x: 2*x\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'x': np.arange(-2, 2, 0.05)\n",
    "})\n",
    "sample_data[\"y\"] = f(sample_data.x)\n",
    "sample_data[\"grad\"] = grad_f(sample_data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.plot(x = 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb26c0",
   "metadata": {},
   "source": [
    "Wniosek, jeśli nie możemy bezpośrednio znaleźć minimum funkcji `f(x)`, zawsze możemy spróbować sprawdzić, gdzie pochodna `f'(x)` jest równa `0` (a dla wielu rozwiązań sprawdź, która z nich jest minimum). To świetnie, ale co zrobić, gdy nie możemy rozwiązać równania `f'(x) == 0`? Żaden problem, zawsze możemy użyć bardzo prostego algorytmu o nazwie **Gradient Descent**:\n",
    "\n",
    "\n",
    "1. Zacznij od wartości początkowej parametrów (x, y, beta, cokolwiek, ...)\n",
    "2. Uaktualnij parametry formułą `param_new := param_old - LR * f'(param_old)` gdzie `LR` jest hiperparametrem zwanym **learning rate**. Wypróbujmy to dla naszej funkcji. Wypróbuj różne wartości learning rate, takie jak `0.1`, `0.01` i `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.345\n",
    "lr = 0.1\n",
    "epochs = 30 # Liczba epok, czyli liczba aktualizacji wag\n",
    "\n",
    "def GD_ordinary_fun(f, grad_f, x0, lr, epochs):\n",
    "    x = x0\n",
    "    results = pd.DataFrame({\n",
    "        'x': [x0],\n",
    "        'y': [f(x0)],\n",
    "        'grad': [grad_f(x0)]\n",
    "    })\n",
    "    for i in range(1, epochs + 1):\n",
    "        x = x - lr * grad_f(x) # Gradient Descent\n",
    "        print(\"Updated x value:\", round(x, 8), \". Updated f(x) value:\", round(f(x), 8))\n",
    "        results = results.append(pd.DataFrame({\n",
    "            'x': [x],\n",
    "            'y': [f(x)],\n",
    "            'grad': [grad_f(x)]\n",
    "    }))\n",
    "    return results\n",
    "\n",
    "task1 = GD_ordinary_fun(f, grad_f, x0, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sample_data.plot(x = 'x')\n",
    "task1.plot(ax = ax, x = 'x', y = 'y', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e63559",
   "metadata": {},
   "source": [
    "Jeśli teraz możemy zaimplementować Gradient Descent dla prostej funkcji z jednym parametrem, możemy spróbować rozwiązać podstawowy problem uczenia maszynowego - regresję liniową."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "sample_data = pd.DataFrame({\n",
    "    'x': np.random.uniform(-3, 3, 50)\n",
    "})\n",
    "sample_data[\"y\"] = sample_data.x + np.random.normal(3, 1.2, 50)\n",
    "sample_data[\"intercept\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.plot(x = 'x', y = 'y', kind = 'scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee686413",
   "metadata": {},
   "source": [
    "Naszym zadaniem jest znalezienie parametrów `b0` i `b1` modelu liniowego `y = b0 + b1*x` takich aby błąd średniokwadratowy (mean squared error) był minimalny. Zanim zaczniemy, sprawdźmy rozwiązanie z pythona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = np.array(sample_data[\"x\"]).reshape(-1, 1)\n",
    "y = np.array(sample_data[\"y\"]).reshape(-1, 1)\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(\"b0\", reg.intercept_)\n",
    "print(\"b1\", reg.coef_)\n",
    "print(\"MSE:\", mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c54ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.array(sample_data[\"intercept\"]).reshape(-1, 1), X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae45914",
   "metadata": {},
   "source": [
    "Teraz musimy zaimplementować MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01735661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(beta, X, y):\n",
    "    betaX = np.matmul(beta, X.transpose())\n",
    "    return np.mean((betaX - y[:,0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f526b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(np.array([2.89793207, 1.03226923]), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52e785",
   "metadata": {},
   "source": [
    "oraz pochodną MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_grad(beta, X, y):\n",
    "    betaX = np.matmul(beta, X.transpose())\n",
    "    return 2*np.matmul(betaX - y[:,0], X)/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f25c68",
   "metadata": {},
   "source": [
    "Szczerze mówiąc, nie musimy tutaj używać Gradient Descent, równanie `MSE_grad == 0` ma rozwiązanie numeryczne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(np.matmul(np.linalg.inv(np.matmul(X.transpose(), X)), X.transpose()), y[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e642281",
   "metadata": {},
   "source": [
    "Ale powiedzmy, że naprawdę chcemy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta00 = np.array([5, -0.2])\n",
    "lr = 0.1\n",
    "epochs = 30 # Liczba epok, czyli liczba aktualizacji wag (po całym datasecie)\n",
    "\n",
    "def GD_linear_regression(beta00, X, y, lr, epochs):\n",
    "    beta = beta00\n",
    "    results = pd.DataFrame({\n",
    "        'b0': [beta00[0]],\n",
    "        'b1': [beta00[1]],\n",
    "        'mse': [MSE(beta00, X, y)],\n",
    "        'epoch': [0]\n",
    "    })\n",
    "    for i in range(1, epochs + 1):\n",
    "        beta = beta - lr * MSE_grad(beta, X, y) # Gradient Descent\n",
    "        print(\"Updated b0 value:\", round(beta[0], 8), \". Updated b1 value:\", round(beta[1], 8), \". Updated MSE value:\", round(MSE(beta, X, y), 8))\n",
    "        results = results.append(pd.DataFrame({\n",
    "            'b0': [beta[0]],\n",
    "            'b1': [beta[1]],\n",
    "            'mse': [MSE(beta, X, y)],\n",
    "            'epoch': [i]\n",
    "    }))\n",
    "    return results\n",
    "\n",
    "task2 = GD_linear_regression(beta00, X, y, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77896989",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\n",
    "    'x': sample_data.x\n",
    "})\n",
    "prediction[\"start\"] = [beta00[0] + beta00[1]*x for x in prediction.x]\n",
    "prediction[\"end\"] = [task2.b0.iloc[-1] + task2.b1.iloc[-1]*x for x in prediction.x]\n",
    "ax = sample_data.plot(x = 'x', y = 'y', kind = 'scatter')\n",
    "prediction.plot(ax = ax, x = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2.plot(x = 'epoch', y = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce43266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "beta_grid = list(itertools.product(np.arange(task2.b0.iloc[-1]-2, task2.b0.iloc[-1] +2, 0.1),\n",
    "           np.arange(task2.b1.iloc[-1]-2, task2.b1.iloc[-1] +2, 0.1)))\n",
    "beta_grid = pd.DataFrame({\n",
    "    'b0': [x[0] for x in beta_grid],\n",
    "    'b1': [x[1] for x in beta_grid]\n",
    "})\n",
    "beta_grid[\"mse\"] = [MSE(np.array([b0, b1]), X, y) for b0, b1 in zip(beta_grid.b0, beta_grid.b1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(beta_grid.pivot('b0', 'b1', 'mse'), square=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2.plot(x = 'b1', y = 'b0', marker = \">\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8aedeb",
   "metadata": {},
   "source": [
    "Wiemy teraz, czym jest Gradient Descent i jak go używać do rozwiązywania problemów ML. W rzeczywistości często będziemy używać zaawansowanej wersji gradientu o nazwie **Stochastic Gradient Descent** lub **SGD** w skrócie. Wprowadzimy jedną małą zmianę w naszym algorytmie. Jak pamiętasz, nasz gradient MSE `MSE_grad` bierze pod uwagę macierz obliczeń `X` i vecotr `y` i mnoży je na różne sposoby. Załóżmy na chwilę, że mamy miliony obserwacji, te mnożenia mogą zająć dużo czasu i pamięci, może być nawet niemożliwe. Istnieje proste rozwiązanie tego problemu. Nasze dane możemy podzielić na tzw. **batche**. Jeśli mamy np. 50 obserwacji, możemy je podzielić na 5 batchy - po 10 obserwacji każda. Po tych 5 batchach SGD zobaczy wszystkie obserwacje, które mieliśmy - minie pierwsza **epoka** i proces rozpocznie się od początku. Można o tym myśleć jako o dodatkowej pętli nad partiami wewnątrz pętli epok z implementacji GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta00 = np.array([5, -0.2])\n",
    "lr = 0.1\n",
    "epochs = 30 # Liczba epok, czyli liczba aktualizacji wag (po całym datasecie)\n",
    "batch_size = 3 # Liczba podzbiorów w datasecie\n",
    "\n",
    "def SGD_linear_regression(beta00, X, y, lr, epochs, batch_size):\n",
    "    beta = beta00\n",
    "    results = pd.DataFrame({\n",
    "        'b0': [beta00[0]],\n",
    "        'b1': [beta00[1]],\n",
    "        'mse': [MSE(beta00, X, y)],\n",
    "        'epoch': [0]\n",
    "    })\n",
    "    batches_per_epoch = int(np.ceil(y.shape[0] / batch_size))\n",
    "    for i in range(1, epochs + 1):\n",
    "        for b in range(1, batches_per_epoch + 1):\n",
    "            X_b = X[((b - 1) * batch_size ):min((b * batch_size + 1), y.shape[0]), :]\n",
    "            y_b = y[((b - 1) * batch_size ):min((b * batch_size + 1), y.shape[0]), :]\n",
    "            beta = beta - lr * MSE_grad(beta, X_b, y_b) # Stochastic Gradient Descent\n",
    "            print(\"Updated b0 value:\", round(beta[0], 8), \". Updated b1 value:\", round(beta[1], 8), \". Updated MSE value:\", round(MSE(beta, X, y), 8))\n",
    "            results = results.append(pd.DataFrame({\n",
    "                'b0': [beta[0]],\n",
    "                'b1': [beta[1]],\n",
    "                'mse': [MSE(beta, X, y)],\n",
    "                'epoch': [i + b / batches_per_epoch]\n",
    "    }))\n",
    "    return results\n",
    "\n",
    "task3 = SGD_linear_regression(beta00, X, y, lr, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcffe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({\n",
    "    'x': sample_data.x\n",
    "})\n",
    "prediction[\"start\"] = [beta00[0] + beta00[1]*x for x in prediction.x]\n",
    "prediction[\"end\"] = [task3.b0.iloc[-1] + task3.b1.iloc[-1]*x for x in prediction.x]\n",
    "ax = sample_data.plot(x = 'x', y = 'y', kind = 'scatter')\n",
    "prediction.plot(ax = ax, x = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407818b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3.plot(x = 'epoch', y = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beff797",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3.plot(x = 'b1', y = 'b0', marker = \">\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b338cd4",
   "metadata": {},
   "source": [
    "W przypadku SGD obliczenia potrzebne do aktualizacji parametrów są szybsze niż w GD, ale SGD może potrzebować więcej kroków niż GD, aby zminimalizować funkcję. Jest jeszcze jedna ważna przewaga SGD nad GD - SGD może „wydostać się” z lokalnego minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0870d",
   "metadata": {},
   "source": [
    "Kolejnym krokiem w zrozumieniu procesu optymalizacji będzie implementacja SGD dla regresji logistycznej. W przypadku regresji logistycznej nie ma rozwiązania nuerycznego, więc musimy użyć jakiegoś algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f79e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spirals = pd.read_csv(\"data/spirals.csv\")\n",
    "spirals[\"intercept\"] = 1\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = {0:'red', 1:'blue'}\n",
    "\n",
    "grouped = spirals.groupby('class')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f392288",
   "metadata": {},
   "source": [
    "Jak zawsze możemy sprawdzić rozwiązanie w Python. Python używa algorytmu scoringu Fishera lub algorytmu scoringu Newtona - algorytmy te wykorzystują nie tylko pierwszą, ale także drugą pochodną do aktualizacji parametrów. Korzystanie z drugiej pochodnej ma zalety, ale obliczenia są naprawdę czasochłonne i pochłaniają pamięć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43937a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = np.array(spirals[[\"x\", \"y\"]]).reshape(-1, 2)\n",
    "y = np.array(spirals[\"class\"]).reshape(-1, 1)\n",
    "\n",
    "reg = LogisticRegression().fit(X, y)\n",
    "print(\"b0\", reg.intercept_)\n",
    "print(\"b1 b2\", reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93995902",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.array(spirals[\"intercept\"]).reshape(-1, 1), X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417bbe5",
   "metadata": {},
   "source": [
    "Następnym krokiem jest zaimplementowanie funkcji **sigmoid** używanej w regresji logistycznej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aea114",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d839cb6",
   "metadata": {},
   "source": [
    "i jej pochodnej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_grad = lambda x: sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14e486",
   "metadata": {},
   "source": [
    "W przypadku klasyfikacji binarnej naszą funkcją straty będzie **binary crossentropy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(beta, X, y):\n",
    "    z = sigmoid(np.matmul(beta, X.transpose()))\n",
    "    return -np.mean(y * np.log(z) + (1 - y) * np.log(1 - z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffc5ff",
   "metadata": {},
   "source": [
    "Potrzebujemy również gradientu. Tutaj zastosujemy tak zwaną **regułę łańcuchową** dla pochodnych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16503667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy_grad(beta, X, y):\n",
    "    z = sigmoid(np.matmul(beta, X.transpose()))\n",
    "    dL = (-y[:,0] / z - (1 - y[:,0]) / (z - 1)) / y.shape[0]\n",
    "    dV = sigmoid_grad(np.matmul(beta, X.transpose()))\n",
    "    dx = X\n",
    "    return np.matmul((dL * dV), dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bbd66",
   "metadata": {},
   "source": [
    "Teraz mamy wszystko do zaimplementowania SGD do regresji logistycznej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b4636",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta00 = np.array([0.3, 1, -0.2])\n",
    "lr = 0.1\n",
    "epochs = 50 # Liczba epok, czyli liczba aktualizacji wag (po całym datasecie)\n",
    "batch_size = 20 # Liczba podzbiorów w datasecie\n",
    "\n",
    "def SGD_logistic_regression(beta00, X, y, lr, epochs, batch_size):\n",
    "    beta = beta00\n",
    "    results = pd.DataFrame({\n",
    "        'b0': [beta00[0]],\n",
    "        'b1': [beta00[1]],\n",
    "        'b2': [beta00[2]],\n",
    "        'logloss': [binary_crossentropy(beta00, X, y)],\n",
    "        'epoch': [0]\n",
    "    })\n",
    "    batches_per_epoch = int(np.ceil(y.shape[0] / batch_size))\n",
    "    for i in range(1, epochs + 1):\n",
    "        for b in range(1, batches_per_epoch + 1):\n",
    "            X_b = X[((b - 1) * batch_size ):min((b * batch_size + 1), y.shape[0]), :]\n",
    "            y_b = y[((b - 1) * batch_size ):min((b * batch_size + 1), y.shape[0]), :]\n",
    "            beta = beta - lr * binary_crossentropy_grad(beta, X_b, y_b) # Stochastic Gradient Descent\n",
    "            print(\"Updated b0 value:\", round(beta[0], 8), \". Updated b1 value:\", round(beta[1], 8), \". Updated b2 value:\", round(beta[2], 8), \". Updated Logloss value:\", round(binary_crossentropy(beta, X, y), 8))\n",
    "            results = results.append(pd.DataFrame({\n",
    "                'b0': [beta[0]],\n",
    "                'b1': [beta[1]],\n",
    "                'b2': [beta[2]],\n",
    "                'logloss': [binary_crossentropy(beta, X, y)],\n",
    "                'epoch': [i + b / batches_per_epoch]\n",
    "    }))\n",
    "    return results\n",
    "\n",
    "task4 = SGD_logistic_regression(beta00, X, y, lr, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b185466",
   "metadata": {},
   "outputs": [],
   "source": [
    "task4.plot(x = 'epoch', y = 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d952fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_grid = list(itertools.product(np.arange(spirals.x.min(), spirals.x.max(), 0.1),\n",
    "           np.arange(spirals.y.min(), spirals.y.max(), 0.1)))\n",
    "xy_grid = pd.DataFrame({\n",
    "    'x': [z[0] for z in xy_grid],\n",
    "    'y': [z[1] for z in xy_grid]\n",
    "})\n",
    "xy_grid[\"class\"] = [task4.b0.iloc[-1] + task4.b1.iloc[-1]*x + task4.b2.iloc[-1]*y for x, y in zip(xy_grid.x, xy_grid.y)]\n",
    "xy_grid[\"class\"] = sigmoid(xy_grid[\"class\"]) > 0.5\n",
    "xy_grid[\"class\"] = xy_grid[\"class\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f927b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = {0:'red', 1:'blue'}\n",
    "\n",
    "grouped = xy_grid.groupby('class')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c2686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
